\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Definitions}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Figures for exogenous network case}{1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parameters for runs for the three cases above.\relax }}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Case 1 - (I) - equal: Average final action of agents as a function of the density of the ER network (rho). The average final action is defined as: $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data shown is averaged over $S$ (1000) simulations per network density $\rho $. For $\rho = 0$ the agents' actions do not synchronize as they are solely determined by their private belief. As the network density is increased social learning leads to action synchronization.\relax }}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Case 1 - (I) - equal : Fraction of simulations per parameter configuration $S$ (1000) in which agents synchronize on the state non matching action (more than 80\% of agents choose state non-matching action) as a function of network density (rho). We distinguish three cases: (1) unconditional: we compute the fraction based on the full sample $S$. (2) conditional $xi \leq 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N \leq 0.5$, i.e. when the agents start with a state matching action. (3) conditional $xi > 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N > 0.5$, i.e. when the agents start with a state non matching action. \relax }}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Case 1 - (I) - equal: We plot the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N$ vs. the average final action $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data points are averages over $S$ (1000) simulations and all network densities $\rho $ (20 values equally distributed over the interval $[0,0.95]$. The color code indicates the frequency with which a point occurs in the sample (total size $20 \times 1000$), the scale of the color code is logarithmic of base 10.\relax }}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Case 1 - (I) - neighbor: Average final action of agents as a function of the density of the ER network (rho). The average final action is defined as: $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data shown is averaged over $S$ (1000) simulations per network density $\rho $. For $\rho = 0$ the agents' actions do not synchronize as they are solely determined by their private belief. As the network density is increased social learning leads to action synchronization.\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Case 1 - (I) - neighbor: Fraction of simulations per parameter configuration $S$ (1000) in which agents synchronize on the state non matching action (more than 80\% of agents choose state non-matching action) as a function of network density (rho). We distinguish three cases: (1) unconditional: we compute the fraction based on the full sample $S$. (2) conditional $xi \leq 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N \leq 0.5$, i.e. when the agents start with a state matching action. (3) conditional $xi > 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N > 0.5$, i.e. when the agents start with a state non matching action. \relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Case 1 - (I) - neighbor: We plot the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N$ vs. the average final action $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data points are averages over $S$ (1000) simulations and all network densities $\rho $ (20 values equally distributed over the interval $[0,0.95]$. The color code indicates the frequency with which a point occurs in the sample (total size $20 \times 1000$), the scale of the color code is logarithmic of base 10.\relax }}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Case 1 - (I) - rel neighbor: Average final action of agents as a function of the density of the ER network (rho). The average final action is defined as: $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data shown is averaged over $S$ (1000) simulations per network density $\rho $. For $\rho = 0$ the agents' actions do not synchronize as they are solely determined by their private belief. As the network density is increased social learning leads to action synchronization.\relax }}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Case 1 - (I) - rel neighbor: Fraction of simulations per parameter configuration $S$ (1000) in which agents synchronize on the state non matching action (more than 80\% of agents choose state non-matching action) as a function of network density (rho). We distinguish three cases: (1) unconditional: we compute the fraction based on the full sample $S$. (2) conditional $xi \leq 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N \leq 0.5$, i.e. when the agents start with a state matching action. (3) conditional $xi > 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N > 0.5$, i.e. when the agents start with a state non matching action. \relax }}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Case 1 - (I) - rel neighbor: We plot the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N$ vs. the average final action $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data points are averages over $S$ (1000) simulations and all network densities $\rho $ (20 values equally distributed over the interval $[0,0.95]$. The color code indicates the frequency with which a point occurs in the sample (total size $20 \times 1000$), the scale of the color code is logarithmic of base 10.\relax }}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Case 2 - (U) - equal: Average final action of agents as a function of the density of the ER network (rho). The average final action is defined as: $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data shown is averaged over $S$ (1000) simulations per network density $\rho $. For $\rho = 0$ the agents' actions do not synchronize as they are solely determined by their private belief. As the network density is increased social learning leads to action synchronization.\relax }}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Case 2 - (U) - equal : Fraction of simulations per parameter configuration $S$ (1000) in which agents synchronize on the state non matching action (more than 80\% of agents choose state non-matching action) as a function of network density (rho). We distinguish three cases: (1) unconditional: we compute the fraction based on the full sample $S$. (2) conditional $xi \leq 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N \leq 0.5$, i.e. when the agents start with a state matching action. (3) conditional $xi > 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N > 0.5$, i.e. when the agents start with a state non matching action. \relax }}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Case 2 - (U) - equal: We plot the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N$ vs. the average final action $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data points are averages over $S$ (1000) simulations and all network densities $\rho $ (20 values equally distributed over the interval $[0,0.95]$. The color code indicates the frequency with which a point occurs in the sample (total size $20 \times 1000$), the scale of the color code is logarithmic of base 10.\relax }}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Case 2 - (U) - neighbor: Average final action of agents as a function of the density of the ER network (rho). The average final action is defined as: $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data shown is averaged over $S$ (1000) simulations per network density $\rho $. For $\rho = 0$ the agents' actions do not synchronize as they are solely determined by their private belief. As the network density is increased social learning leads to action synchronization.\relax }}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Case 2 - (U) - neighbor: Fraction of simulations per parameter configuration $S$ (1000) in which agents synchronize on the state non matching action (more than 80\% of agents choose state non-matching action) as a function of network density (rho). We distinguish three cases: (1) unconditional: we compute the fraction based on the full sample $S$. (2) conditional $xi \leq 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N \leq 0.5$, i.e. when the agents start with a state matching action. (3) conditional $xi > 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N > 0.5$, i.e. when the agents start with a state non matching action. \relax }}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Case 2 - (U) - neighbor: We plot the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N$ vs. the average final action $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data points are averages over $S$ (1000) simulations and all network densities $\rho $ (20 values equally distributed over the interval $[0,0.95]$. The color code indicates the frequency with which a point occurs in the sample (total size $20 \times 1000$), the scale of the color code is logarithmic of base 10.\relax }}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Case 2 - (U) - rel neighbor: Average final action of agents as a function of the density of the ER network (rho). The average final action is defined as: $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data shown is averaged over $S$ (1000) simulations per network density $\rho $. For $\rho = 0$ the agents' actions do not synchronize as they are solely determined by their private belief. As the network density is increased social learning leads to action synchronization.\relax }}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Case 2 - (U) - rel neighbor: Fraction of simulations per parameter configuration $S$ (1000) in which agents synchronize on the state non matching action (more than 80\% of agents choose state non-matching action) as a function of network density (rho). We distinguish three cases: (1) unconditional: we compute the fraction based on the full sample $S$. (2) conditional $xi \leq 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N \leq 0.5$, i.e. when the agents start with a state matching action. (3) conditional $xi > 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N > 0.5$, i.e. when the agents start with a state non matching action. \relax }}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Case 2 - (U) - rel neighbor: We plot the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N$ vs. the average final action $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data points are averages over $S$ (1000) simulations and all network densities $\rho $ (20 values equally distributed over the interval $[0,0.95]$. The color code indicates the frequency with which a point occurs in the sample (total size $20 \times 1000$), the scale of the color code is logarithmic of base 10.\relax }}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Case 3 - (H) - het: Average final action of agents as a function of the probability of being informed $p$. The average final action is defined as: $x_F = \DOTSB \sum@ \slimits@ _j x_j(T)/N$. Data shown is averaged over $S$ (1000) simulations per probability of being informed $p$.\relax }}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Case 3 - (H) - het: Fraction of simulations per parameter configuration $S$ (1000) in which agents synchronize on the state non matching action (more than 80\% of agents choose state non-matching action) as a function of the probability of being informed $p$. We distinguish three cases: (1) unconditional: we compute the fraction based on the full sample $S$. (2) conditional $xi \leq 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N \leq 0.5$, i.e. when the agents start with a state matching action. (3) conditional $xi > 0.5$: we compute the fraction based on the sub-set of simulations in which the average initial action $x_i = \DOTSB \sum@ \slimits@ _j x_j(0)/N > 0.5$, i.e. when the agents start with a state non matching action. \relax }}{13}}
